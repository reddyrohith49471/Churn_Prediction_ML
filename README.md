# ğŸ“Š Telecom Customer Churn Prediction â€“ End-to-End MLOps System

An end-to-end **Machine Learning and MLOps** project that predicts customer churn using the **Telecom Customer Churn dataset**, covering the complete ML lifecycle including **model experimentation, tracking, deployment, drift monitoring, and automated alerting**.

This project focuses not only on building a high-performing model but also on solving **real-world production challenges** such as data drift, prediction drift, and model drift.

---

## ğŸ“Œ Problem Statement

Customer churn prediction identifies customers who are likely to stop using a service.  
The dataset used in this project is **highly imbalanced**, making accuracy-only evaluation misleading and requiring careful metric selection and continuous monitoring.

---

## ğŸ“‚ Dataset

- **Dataset**: Telecom Customer Churn  
- **Source**: Kaggle *(https://www.kaggle.com/datasets/blastchar/telco-customer-churn)*  
- **Target Variable**: `Churn`  
- **Key Challenge**: Severe class imbalance  

---

## ğŸ§  Project Methodology

### 1ï¸âƒ£ Exploratory Data Analysis & Preprocessing
- Performed detailed exploratory data analysis using multiple visualization techniques
- Analyzed feature distributions, correlations, and imbalance
- Applied multiple encoding strategies for categorical features
- Handled missing values and inconsistent data types

---

### 2ï¸âƒ£ Model Experimentation
- Trained multiple baseline machine learning models
- Compared performance using **precision, recall, and F1-score**
- Avoided random model selection by running structured experiments

---

### 3ï¸âƒ£ Hyperparameter Tuning
- Tuned selected models using **RandomizedSearchCV**
- Compared baseline vs tuned models
- Identified the best model based on imbalance-aware metrics

---

### 4ï¸âƒ£ Experiment Tracking with MLflow
- Tracked all experiments using **MLflow**
- Logged model parameters, evaluation metrics, and model artifacts
- Compared experiments visually in the MLflow UI
- **Tuned Random Forest Classifier** performed best

âœ… **Final model used in production: Tuned Random Forest Classifier**

---

## ğŸ— System Architecture

The following diagram illustrates the complete architecture, including inference, monitoring, and alerting.

```mermaid
flowchart TD
    A[User / Client] --> B[FastAPI Inference API]

    B --> C[Model Loader]
    C --> D[Tuned Random Forest Model]

    B --> E[Inference Logger]
    E --> F[Live Data Storage]

    F --> G[Drift Detection Engine]

    G --> H[Data Drift<br/>Evidently]
    G --> I[Prediction Drift<br/>Chi-Square Test]
    G --> J[Model Drift<br/>RFC Metrics Comparison]

    H --> K[Drift Reports<br/>HTML / JSON]
    I --> K
    J --> K

    K --> L[Email Notification System]
    L --> M[Alerts to Stakeholders]

    
```

## ğŸš€ Model Serving & Applications

### ğŸ”¹ Streamlit (Initial Stage)
- Built a **Streamlit application** for:
  - Interactive predictions
  - Quick validation of model behavior

### ğŸ”¹ FastAPI (Production-ready)
- Implemented a **FastAPI backend** for:
  - Scalable inference
  - API-based predictions
- Designed to support future extensions like:
  - Segment-wise reports
  - A/B testing endpoints

---

## ğŸ“‰ Drift Monitoring & MLOps

One of the core highlights of this project is **post-deployment monitoring**.

### ğŸ” Drift Types Implemented

#### âœ… Data Drift
- Implemented using **Evidently**
- Detects feature distribution shifts between:
  - Reference (training) data
  - Live production data
- Generates **HTML + JSON reports**
- Reports are automatically saved

#### âœ… Prediction Drift
- Implemented using **Chi-Square statistical test**
- Detects shifts in prediction distribution over time

#### âœ… Model Drift
- Uses the **final tuned Random Forest model**
- Monitors degradation in model behavior due to data changes

---

### ğŸ“§ Automated Alerts
- If **any drift is detected**:
  - An **email alert is automatically triggered**
  - Drift reports are **attached to the email**
- SMTP credentials handled securely using environment variables

---

## â˜ï¸ Deployment

- Deployed on **Render**
- Uses:
  - Environment variables
  - Secure secrets management
- Supports:
  - Automatic scaling
  - Continuous availability

Deployment link: *(https://churn-prediction-ml-zvrg.onrender.com/predict)*

---

## ğŸ—‚ Project Structure
```
.
â”œâ”€â”€ artifacts
â”‚   â””â”€â”€ rfc_tuned_model.joblib
â”œâ”€â”€ config
â”‚   â”œâ”€â”€ schemas.yaml
â”‚   â””â”€â”€ settings.py
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ incoming
â”‚   â”‚   â””â”€â”€ live_data.csv
â”‚   â””â”€â”€ reference
â”‚       â””â”€â”€ WA_Fn-UseC_-Telco-Customer-Churn.csv
â”œâ”€â”€ drift_runners
â”‚   â””â”€â”€ run_drift.py
â”œâ”€â”€ fastapi_app.py
â”œâ”€â”€ helper.py
â”œâ”€â”€ notebooks
â”‚   â””â”€â”€ experiment-notebook.ipynb
â”œâ”€â”€ render.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ core
â”‚   â”œâ”€â”€ logging
â”‚   â”œâ”€â”€ monitoring
â”‚   â”‚   â”œâ”€â”€ drift_jobs
â”‚   â”‚   â”œâ”€â”€ reports_builder
â”‚   â”‚   â”œâ”€â”€ handling_drift
â”‚   â”‚   â””â”€â”€ email_notifications
â”‚   â””â”€â”€ utils
â”œâ”€â”€ streamlit_app.py
â”œâ”€â”€ templates
â”‚   â””â”€â”€ index.html
â””â”€â”€ test.py
```

---

## ğŸ›  Tech Stack

- **Python**
- **Scikit-learn**
- **MLflow**
- **FastAPI**
- **Streamlit**
- **Evidently**
- **Pandas, NumPy, Matplotlib**
- **Render (Deployment)**

---

## ğŸ”® Future Enhancements

Planned extensions include:
- **Segment-wise drift reports**
- **A/B testing**
  - Threshold tuning at model level
  - Threshold tuning at feature level
- Advanced business-driven monitoring metrics

---

## ğŸ‘¨â€ğŸ’» Author

**Reddy Rohith Kosinepalli**    



